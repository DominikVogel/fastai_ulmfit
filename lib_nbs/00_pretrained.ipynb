{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained\n",
    "\n",
    "> fast.ai ULMFiT helpers to easily use pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from fastai.text.all import SentencePieceTokenizer, language_model_learner, \\\n",
    "                            text_classifier_learner, untar_data, Path, patch, \\\n",
    "                            LMLearner, os, pickle, shutil, AWD_LSTM, accuracy, \\\n",
    "                            Perplexity, delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_config(path):\n",
    "    with open(path/'model.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_pretrained_model(url):\n",
    "    fname = f\"{url.split('/')[-1]}.tgz\"\n",
    "    path = untar_data(url, fname=fname, c_key='model')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_direction(backwards):\n",
    "    return 'bwd' if backwards else 'fwd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert(_get_direction(backwards=False) == 'fwd')\n",
    "assert(_get_direction(backwards=True) == 'bwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `model` and `vocab` files from path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_files(path, backwards=False):\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    try: \n",
    "        model_path = path/direction\n",
    "        model_file = list(model_path.glob(f'*model.pth'))[0]\n",
    "        vocab_file = list(model_path.glob(f'*vocab.pkl'))[0]\n",
    "        fnames = [model_file.absolute(),vocab_file.absolute()]\n",
    "    except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "    fnames = [str(f.parent/f.stem) for f in fnames]\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `tokenizer` from model-config. Tokenizer parameters in `model.json` will be passed to the Tokenizer. As of now SentencePieceTokenizer is hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenizer_from_pretrained(url, backwards=False, **kwargs):\n",
    "    path = _get_pretrained_model(url)\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    tok = None\n",
    "    if config['tokenizer']['class'] == 'SentencePieceTokenizer':\n",
    "        tok = SentencePieceTokenizer(**config['tokenizer']['params'], **kwargs)\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `langauge_model_learner` from pretrained model-URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(language_model_learner)\n",
    "def language_model_from_pretrained(dls, url=None, backwards=False, metrics=None, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_pretrained_model(url)\n",
    "    fnames = _get_model_files(path)\n",
    "    metrics = [accuracy, Perplexity()] if metrics == None else metrics\n",
    "    return language_model_learner(dls, \n",
    "                                  arch, \n",
    "                                  pretrained=True, \n",
    "                                  pretrained_fnames=fnames, \n",
    "                                  metrics=metrics,\n",
    "                                  **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves a trained or fine-tuned language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_path(learn=None, path=None):\n",
    "    path = (learn.path/learn.model_dir) if not path else Path(path)\n",
    "    if not path.exists(): os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the following model files to `path`:\n",
    "- Model (`lm_model.pth`)\n",
    "- Encoder (`lm_encoder.pth`)\n",
    "- Vocab from dataloaders (`lm_vocab.pkl`)\n",
    "- SentencePieceModel (`spm/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def save_lm(x:LMLearner, path=None, with_encoder=True):\n",
    "    path = _get_model_path(x, path)\n",
    "    x.to_fp32()\n",
    "    # save model\n",
    "    x.save((path/'lm_model').absolute(), with_opt=False)\n",
    "    \n",
    "    # save encoder\n",
    "    if with_encoder:\n",
    "        x.save_encoder((path/'lm_encoder').absolute())\n",
    "\n",
    "    # save vocab\n",
    "    with open((path/'lm_vocab.pkl').absolute(), 'wb') as f:\n",
    "        pickle.dump(x.dls.vocab, f)\n",
    "        \n",
    "    # copy SPM if path not spm path\n",
    "    spm_path = Path(x.dls.tok.cache_dir)\n",
    "    if path.absolute() != spm_path.absolute():\n",
    "        target_path = path/'spm'\n",
    "        if not target_path.exists(): os.makedirs(target_path, exist_ok=True)\n",
    "        shutil.copyfile(spm_path/'spm.model', target_path/'spm.model')\n",
    "        shutil.copyfile(spm_path/'spm.vocab', target_path/'spm.vocab')\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def vocab_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)\n",
    "#    with open((path/'lm_vocab.pkl').absolute(), 'rb') as f:\n",
    "#        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def spm_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `text_classifier_learner` from fine-tuned model path (saved with `learn.save_lm()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(text_classifier_learner)\n",
    "def text_classifier_from_lm(dls, path=None, backwards=False, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_model_path(path=path)\n",
    "    learn = text_classifier_learner(dls, arch, pretrained=False, **kwargs)\n",
    "    learn.load_encoder((path/'lm_encoder').absolute())\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests - Tokenizer, LM and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "url = 'http://localhost:8080/ulmfit-dewiki'\n",
    "tok = tokenizer_from_pretrained(url)\n",
    "assert(tok.vocab_sz == 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.456911</td>\n",
       "      <td>6.521295</td>\n",
       "      <td>0.169497</td>\n",
       "      <td>679.457458</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_lm_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, is_lm=True, backwards=backwards)),\n",
    "                    get_x=ColReader('text'), \n",
    "                    splitter=RandomSplitter(valid_pct=0.1, seed=42))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = language_model_from_pretrained(dls, url=url, backwards=backwards)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "path = learn.save_lm()\n",
    "vocab = learn.dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.746191</td>\n",
       "      <td>0.686059</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5208, 0.4792],\n",
       "         [0.4542, 0.5458],\n",
       "         [0.5424, 0.4576],\n",
       "         [0.5126, 0.4874],\n",
       "         [0.5279, 0.4721],\n",
       "         [0.5018, 0.4982],\n",
       "         [0.5503, 0.4497],\n",
       "         [0.5119, 0.4881],\n",
       "         [0.5376, 0.4624],\n",
       "         [0.5206, 0.4794],\n",
       "         [0.5527, 0.4473],\n",
       "         [0.4942, 0.5058],\n",
       "         [0.4967, 0.5033],\n",
       "         [0.5023, 0.4977],\n",
       "         [0.4966, 0.5034],\n",
       "         [0.5292, 0.4708],\n",
       "         [0.4937, 0.5063],\n",
       "         [0.5089, 0.4911],\n",
       "         [0.4843, 0.5157],\n",
       "         [0.5148, 0.4852],\n",
       "         [0.4945, 0.5055],\n",
       "         [0.5097, 0.4903],\n",
       "         [0.4925, 0.5075],\n",
       "         [0.5141, 0.4859],\n",
       "         [0.5034, 0.4966],\n",
       "         [0.5540, 0.4460],\n",
       "         [0.5136, 0.4864],\n",
       "         [0.5497, 0.4503],\n",
       "         [0.5158, 0.4842],\n",
       "         [0.4923, 0.5077],\n",
       "         [0.5600, 0.4400],\n",
       "         [0.4952, 0.5048],\n",
       "         [0.5106, 0.4894],\n",
       "         [0.4843, 0.5157],\n",
       "         [0.5156, 0.4844],\n",
       "         [0.5331, 0.4669],\n",
       "         [0.5203, 0.4797],\n",
       "         [0.5552, 0.4448],\n",
       "         [0.5057, 0.4943],\n",
       "         [0.5359, 0.4641],\n",
       "         [0.4891, 0.5109],\n",
       "         [0.4744, 0.5256],\n",
       "         [0.5350, 0.4650],\n",
       "         [0.5409, 0.4591],\n",
       "         [0.4773, 0.5227],\n",
       "         [0.5200, 0.4800],\n",
       "         [0.4556, 0.5444],\n",
       "         [0.5334, 0.4666],\n",
       "         [0.4989, 0.5011],\n",
       "         [0.4724, 0.5276],\n",
       "         [0.4792, 0.5208],\n",
       "         [0.5182, 0.4818],\n",
       "         [0.4997, 0.5003],\n",
       "         [0.5057, 0.4943],\n",
       "         [0.5510, 0.4490],\n",
       "         [0.5119, 0.4881],\n",
       "         [0.5045, 0.4955],\n",
       "         [0.5202, 0.4798],\n",
       "         [0.4711, 0.5289],\n",
       "         [0.5464, 0.4536],\n",
       "         [0.4824, 0.5176],\n",
       "         [0.4763, 0.5237],\n",
       "         [0.5204, 0.4796],\n",
       "         [0.5704, 0.4296],\n",
       "         [0.4931, 0.5069],\n",
       "         [0.4852, 0.5148],\n",
       "         [0.4724, 0.5276],\n",
       "         [0.5216, 0.4784],\n",
       "         [0.4927, 0.5073],\n",
       "         [0.5494, 0.4506],\n",
       "         [0.5137, 0.4863],\n",
       "         [0.4694, 0.5306],\n",
       "         [0.4993, 0.5007],\n",
       "         [0.5098, 0.4902],\n",
       "         [0.5228, 0.4772],\n",
       "         [0.5155, 0.4845],\n",
       "         [0.4857, 0.5143],\n",
       "         [0.5041, 0.4959],\n",
       "         [0.5062, 0.4938],\n",
       "         [0.5420, 0.4580],\n",
       "         [0.4980, 0.5020],\n",
       "         [0.5100, 0.4900],\n",
       "         [0.5185, 0.4815],\n",
       "         [0.5233, 0.4767],\n",
       "         [0.4773, 0.5227],\n",
       "         [0.4621, 0.5379],\n",
       "         [0.5243, 0.4757],\n",
       "         [0.4556, 0.5444],\n",
       "         [0.4998, 0.5002],\n",
       "         [0.5254, 0.4746],\n",
       "         [0.4423, 0.5577],\n",
       "         [0.5275, 0.4725],\n",
       "         [0.4699, 0.5301],\n",
       "         [0.5060, 0.4940],\n",
       "         [0.4768, 0.5232],\n",
       "         [0.4899, 0.5101],\n",
       "         [0.5267, 0.4733],\n",
       "         [0.5476, 0.4524],\n",
       "         [0.4479, 0.5521],\n",
       "         [0.5131, 0.4869],\n",
       "         [0.5063, 0.4937],\n",
       "         [0.4908, 0.5092],\n",
       "         [0.5191, 0.4809],\n",
       "         [0.5207, 0.4793],\n",
       "         [0.5099, 0.4901],\n",
       "         [0.5262, 0.4738],\n",
       "         [0.5023, 0.4977],\n",
       "         [0.5429, 0.4571],\n",
       "         [0.5002, 0.4998],\n",
       "         [0.5052, 0.4948],\n",
       "         [0.4801, 0.5199],\n",
       "         [0.5110, 0.4890],\n",
       "         [0.4711, 0.5289],\n",
       "         [0.4966, 0.5034],\n",
       "         [0.4723, 0.5277],\n",
       "         [0.5188, 0.4812],\n",
       "         [0.5073, 0.4927],\n",
       "         [0.5142, 0.4858],\n",
       "         [0.5508, 0.4492],\n",
       "         [0.5171, 0.4829],\n",
       "         [0.5190, 0.4810],\n",
       "         [0.5446, 0.4554],\n",
       "         [0.4861, 0.5139],\n",
       "         [0.5425, 0.4575],\n",
       "         [0.4865, 0.5135],\n",
       "         [0.5443, 0.4557],\n",
       "         [0.5388, 0.4612],\n",
       "         [0.5040, 0.4960],\n",
       "         [0.4650, 0.5350],\n",
       "         [0.5176, 0.4824],\n",
       "         [0.5301, 0.4699],\n",
       "         [0.5283, 0.4717],\n",
       "         [0.5282, 0.4718],\n",
       "         [0.5239, 0.4761],\n",
       "         [0.4574, 0.5426],\n",
       "         [0.4935, 0.5065],\n",
       "         [0.5103, 0.4897],\n",
       "         [0.5225, 0.4775],\n",
       "         [0.4797, 0.5203],\n",
       "         [0.5069, 0.4931],\n",
       "         [0.4592, 0.5408],\n",
       "         [0.5204, 0.4796],\n",
       "         [0.5194, 0.4806],\n",
       "         [0.4845, 0.5155],\n",
       "         [0.4833, 0.5167],\n",
       "         [0.4960, 0.5040],\n",
       "         [0.5252, 0.4748],\n",
       "         [0.5477, 0.4523],\n",
       "         [0.5075, 0.4925],\n",
       "         [0.4924, 0.5076],\n",
       "         [0.5166, 0.4834],\n",
       "         [0.4985, 0.5015],\n",
       "         [0.4836, 0.5164],\n",
       "         [0.4870, 0.5130],\n",
       "         [0.4975, 0.5025],\n",
       "         [0.5403, 0.4597],\n",
       "         [0.5308, 0.4692],\n",
       "         [0.5372, 0.4628],\n",
       "         [0.5304, 0.4696],\n",
       "         [0.4681, 0.5319],\n",
       "         [0.4902, 0.5098],\n",
       "         [0.5363, 0.4637],\n",
       "         [0.4902, 0.5098],\n",
       "         [0.4833, 0.5167],\n",
       "         [0.4968, 0.5032],\n",
       "         [0.5185, 0.4815],\n",
       "         [0.5409, 0.4591],\n",
       "         [0.4692, 0.5308],\n",
       "         [0.5157, 0.4843],\n",
       "         [0.4985, 0.5015],\n",
       "         [0.5169, 0.4831],\n",
       "         [0.4903, 0.5097],\n",
       "         [0.4834, 0.5166],\n",
       "         [0.4942, 0.5058],\n",
       "         [0.5237, 0.4763],\n",
       "         [0.5263, 0.4737],\n",
       "         [0.5060, 0.4940],\n",
       "         [0.4932, 0.5068],\n",
       "         [0.5520, 0.4480],\n",
       "         [0.5090, 0.4910],\n",
       "         [0.5502, 0.4498],\n",
       "         [0.5464, 0.4536],\n",
       "         [0.5431, 0.4569],\n",
       "         [0.5212, 0.4788],\n",
       "         [0.4943, 0.5057],\n",
       "         [0.5009, 0.4991],\n",
       "         [0.5048, 0.4952],\n",
       "         [0.4819, 0.5181],\n",
       "         [0.4918, 0.5082],\n",
       "         [0.5099, 0.4901],\n",
       "         [0.5005, 0.4995],\n",
       "         [0.4819, 0.5181],\n",
       "         [0.5262, 0.4738],\n",
       "         [0.4994, 0.5006],\n",
       "         [0.5119, 0.4881],\n",
       "         [0.5345, 0.4655],\n",
       "         [0.4553, 0.5447],\n",
       "         [0.5140, 0.4860],\n",
       "         [0.5180, 0.4820],\n",
       "         [0.5370, 0.4630],\n",
       "         [0.4598, 0.5402],\n",
       "         [0.5040, 0.4960],\n",
       "         [0.4927, 0.5073],\n",
       "         [0.5095, 0.4905]]),\n",
       " TensorCategory([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter, CategoryBlock\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_class_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, vocab=vocab, backwards=backwards), CategoryBlock),\n",
    "                    get_x=ColReader('text'), \n",
    "                    get_y=ColReader('label'))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = text_classifier_from_lm(dls, path=path, backwards=backwards)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
