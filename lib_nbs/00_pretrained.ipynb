{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained\n",
    "\n",
    "> fast.ai ULMFiT helpers to easily use pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from fastai.text.all import SentencePieceTokenizer, SpacyTokenizer, language_model_learner, \\\n",
    "                            text_classifier_learner, untar_data, Path, patch, \\\n",
    "                            LMLearner, os, pickle, shutil, AWD_LSTM, accuracy, \\\n",
    "                            Perplexity, delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_config(path):\n",
    "    with open(path/'model.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_pretrained_model(url):\n",
    "    fname = f\"{url.split('/')[-1]}.tgz\"\n",
    "    path = untar_data(url, fname=fname, c_key='model')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_direction(backwards):\n",
    "    return 'bwd' if backwards else 'fwd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_class(classname):\n",
    "    cls = None\n",
    "    if len(classname.split('.')) > 1:\n",
    "        comp = classname.rsplit('.', 1)\n",
    "        imported = import_module(comp[0])\n",
    "        cls = getattr(imported, comp[1])\n",
    "    else:\n",
    "        cls = globals()[classname]\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert(_get_direction(backwards=False) == 'fwd')\n",
    "assert(_get_direction(backwards=True) == 'bwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `model` and `vocab` files from path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_files(path, backwards=False):\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    try: \n",
    "        model_path = path/direction\n",
    "        model_file = list(model_path.glob(f'*model.pth'))[0]\n",
    "        vocab_file = list(model_path.glob(f'*vocab.pkl'))[0]\n",
    "        fnames = [model_file.absolute(),vocab_file.absolute()]\n",
    "    except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "    fnames = [str(f.parent/f.stem) for f in fnames]\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `tokenizer` from model-config. Tokenizer parameters in `model.json` will be passed to the Tokenizer. As of now SentencePiece and Spacy are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenizer_from_pretrained(url, pretrained=False, backwards=False, **kwargs):\n",
    "    path = _get_pretrained_model(url)\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    \n",
    "    if config['tokenizer']['class'] == 'SentencePieceTokenizer':\n",
    "        if pretrained: config['tokenizer']['params']['sp_model'] = path/'spm'/'spm.model'\n",
    "        \n",
    "    tok_cls = _get_class(config['tokenizer']['class'])\n",
    "    tok = tok_cls(**config['tokenizer']['params'])\n",
    "    \n",
    "    return tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `langauge_model_learner` from pretrained model-URL. All parameters will be passed to `language_model_learner`. The following parameters are set automatically: `arch`, `pretrained` and `pretrained_fnames`. By default `accuracy` and `perplexity` are passed as `metrics`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(language_model_learner)\n",
    "def language_model_from_pretrained(dls, url=None, backwards=False, metrics=None, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_pretrained_model(url)\n",
    "    fnames = _get_model_files(path)\n",
    "    metrics = [accuracy, Perplexity()] if metrics == None else metrics\n",
    "    return language_model_learner(dls, \n",
    "                                  arch, \n",
    "                                  pretrained=True, \n",
    "                                  pretrained_fnames=fnames, \n",
    "                                  metrics=metrics,\n",
    "                                  **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_path(learn=None, path=None):\n",
    "    path = (learn.path/learn.model_dir) if not path else Path(path)\n",
    "    if not path.exists(): os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the following model files to `path`:\n",
    "- Model (`lm_model.pth`)\n",
    "- Encoder (`lm_encoder.pth`)\n",
    "- Vocab from dataloaders (`lm_vocab.pkl`)\n",
    "- SentencePieceModel (`spm/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def save_lm(x:LMLearner, path=None, with_encoder=True):\n",
    "    path = _get_model_path(x, path)\n",
    "    x.to_fp32()\n",
    "    # save model\n",
    "    x.save((path/'lm_model').absolute(), with_opt=False)\n",
    "    \n",
    "    # save encoder\n",
    "    if with_encoder:\n",
    "        x.save_encoder((path/'lm_encoder').absolute())\n",
    "\n",
    "    # save vocab\n",
    "    with open((path/'lm_vocab.pkl').absolute(), 'wb') as f:\n",
    "        pickle.dump(x.dls.vocab, f)\n",
    "       \n",
    "    # save tokenizer if SentencePiece is used\n",
    "    if isinstance(x.dls.tok, SentencePieceTokenizer):\n",
    "        # copy SPM if path not spm path\n",
    "        spm_path = Path(x.dls.tok.cache_dir)\n",
    "        if path.absolute() != spm_path.absolute():\n",
    "            target_path = path/'spm'\n",
    "            if not target_path.exists(): os.makedirs(target_path, exist_ok=True)\n",
    "            shutil.copyfile(spm_path/'spm.model', target_path/'spm.model')\n",
    "            shutil.copyfile(spm_path/'spm.vocab', target_path/'spm.vocab')\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def vocab_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)\n",
    "#    with open((path/'lm_vocab.pkl').absolute(), 'rb') as f:\n",
    "#        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def spm_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `text_classifier_learner` from fine-tuned model path (saved with `learn.save_lm()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(text_classifier_learner)\n",
    "def text_classifier_from_lm(dls, path=None, backwards=False, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config / _get_class()\n",
    "    path = _get_model_path(path=path)\n",
    "    learn = text_classifier_learner(dls, arch, pretrained=False, **kwargs)\n",
    "    learn.load_encoder((path/'lm_encoder').absolute())\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests - Tokenizer, LM and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "url = 'http://localhost:8080/ulmfit-dewiki'\n",
    "tok = tokenizer_from_pretrained(url, pretrained=True)\n",
    "assert(tok.vocab_sz == 15000)\n",
    "assert('ulmfit-dewiki/spm/spm.model' in str(tok.sp_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "tok = tokenizer_from_pretrained(url, pretrained=False)\n",
    "assert(tok.sp_model == None)\n",
    "assert(tok.vocab_sz == 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.396237</td>\n",
       "      <td>6.521406</td>\n",
       "      <td>0.169158</td>\n",
       "      <td>679.533264</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_lm_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, is_lm=True, backwards=backwards)),\n",
    "                    get_x=ColReader('text'), \n",
    "                    splitter=RandomSplitter(valid_pct=0.1, seed=42))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = language_model_from_pretrained(dls, url=url, backwards=backwards)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "path = learn.save_lm()\n",
    "vocab = learn.dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.749031</td>\n",
       "      <td>0.692215</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5253, 0.4747],\n",
       "         [0.4646, 0.5354],\n",
       "         [0.5471, 0.4529],\n",
       "         [0.5144, 0.4856],\n",
       "         [0.5184, 0.4816],\n",
       "         [0.4999, 0.5001],\n",
       "         [0.5526, 0.4474],\n",
       "         [0.5139, 0.4861],\n",
       "         [0.5460, 0.4540],\n",
       "         [0.5407, 0.4593],\n",
       "         [0.5530, 0.4470],\n",
       "         [0.4984, 0.5016],\n",
       "         [0.4999, 0.5001],\n",
       "         [0.4947, 0.5053],\n",
       "         [0.4982, 0.5018],\n",
       "         [0.5280, 0.4720],\n",
       "         [0.5149, 0.4851],\n",
       "         [0.5091, 0.4909],\n",
       "         [0.4897, 0.5103],\n",
       "         [0.5194, 0.4806],\n",
       "         [0.4974, 0.5026],\n",
       "         [0.5087, 0.4913],\n",
       "         [0.4885, 0.5115],\n",
       "         [0.5080, 0.4920],\n",
       "         [0.5054, 0.4946],\n",
       "         [0.5558, 0.4442],\n",
       "         [0.5100, 0.4900],\n",
       "         [0.5289, 0.4711],\n",
       "         [0.5193, 0.4807],\n",
       "         [0.5025, 0.4975],\n",
       "         [0.5709, 0.4291],\n",
       "         [0.5011, 0.4989],\n",
       "         [0.5058, 0.4942],\n",
       "         [0.4999, 0.5001],\n",
       "         [0.5175, 0.4825],\n",
       "         [0.5354, 0.4646],\n",
       "         [0.5324, 0.4676],\n",
       "         [0.5649, 0.4351],\n",
       "         [0.4931, 0.5069],\n",
       "         [0.5382, 0.4618],\n",
       "         [0.4808, 0.5192],\n",
       "         [0.4928, 0.5072],\n",
       "         [0.5458, 0.4542],\n",
       "         [0.5373, 0.4627],\n",
       "         [0.4859, 0.5141],\n",
       "         [0.5146, 0.4854],\n",
       "         [0.4758, 0.5242],\n",
       "         [0.5324, 0.4676],\n",
       "         [0.5078, 0.4922],\n",
       "         [0.4900, 0.5100],\n",
       "         [0.4959, 0.5041],\n",
       "         [0.5087, 0.4913],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.5180, 0.4820],\n",
       "         [0.5391, 0.4609],\n",
       "         [0.5183, 0.4817],\n",
       "         [0.5071, 0.4929],\n",
       "         [0.5232, 0.4768],\n",
       "         [0.4924, 0.5076],\n",
       "         [0.5500, 0.4500],\n",
       "         [0.4827, 0.5173],\n",
       "         [0.4898, 0.5102],\n",
       "         [0.5163, 0.4837],\n",
       "         [0.5782, 0.4218],\n",
       "         [0.4924, 0.5076],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.4856, 0.5144],\n",
       "         [0.5203, 0.4797],\n",
       "         [0.5051, 0.4949],\n",
       "         [0.5495, 0.4505],\n",
       "         [0.5224, 0.4776],\n",
       "         [0.4919, 0.5081],\n",
       "         [0.4975, 0.5025],\n",
       "         [0.5219, 0.4781],\n",
       "         [0.5324, 0.4676],\n",
       "         [0.5223, 0.4777],\n",
       "         [0.4876, 0.5124],\n",
       "         [0.5024, 0.4976],\n",
       "         [0.5098, 0.4902],\n",
       "         [0.5506, 0.4494],\n",
       "         [0.5120, 0.4880],\n",
       "         [0.5212, 0.4788],\n",
       "         [0.5071, 0.4929],\n",
       "         [0.5291, 0.4709],\n",
       "         [0.5012, 0.4988],\n",
       "         [0.4750, 0.5250],\n",
       "         [0.5254, 0.4746],\n",
       "         [0.4682, 0.5318],\n",
       "         [0.5063, 0.4937],\n",
       "         [0.5190, 0.4810],\n",
       "         [0.4664, 0.5336],\n",
       "         [0.5115, 0.4885],\n",
       "         [0.4861, 0.5139],\n",
       "         [0.4971, 0.5029],\n",
       "         [0.4884, 0.5116],\n",
       "         [0.4918, 0.5082],\n",
       "         [0.5359, 0.4641],\n",
       "         [0.5372, 0.4628],\n",
       "         [0.4625, 0.5375],\n",
       "         [0.5122, 0.4878],\n",
       "         [0.4989, 0.5011],\n",
       "         [0.4876, 0.5124],\n",
       "         [0.5267, 0.4733],\n",
       "         [0.5167, 0.4833],\n",
       "         [0.5167, 0.4833],\n",
       "         [0.5245, 0.4755],\n",
       "         [0.5004, 0.4996],\n",
       "         [0.5388, 0.4612],\n",
       "         [0.5045, 0.4955],\n",
       "         [0.5161, 0.4839],\n",
       "         [0.4782, 0.5218],\n",
       "         [0.5227, 0.4773],\n",
       "         [0.4878, 0.5122],\n",
       "         [0.5016, 0.4984],\n",
       "         [0.4937, 0.5063],\n",
       "         [0.5130, 0.4870],\n",
       "         [0.5148, 0.4852],\n",
       "         [0.5202, 0.4798],\n",
       "         [0.5506, 0.4494],\n",
       "         [0.5150, 0.4850],\n",
       "         [0.5066, 0.4934],\n",
       "         [0.5511, 0.4489],\n",
       "         [0.5058, 0.4942],\n",
       "         [0.5529, 0.4471],\n",
       "         [0.5056, 0.4944],\n",
       "         [0.5533, 0.4467],\n",
       "         [0.5385, 0.4615],\n",
       "         [0.5192, 0.4808],\n",
       "         [0.4881, 0.5119],\n",
       "         [0.5224, 0.4776],\n",
       "         [0.5286, 0.4714],\n",
       "         [0.5270, 0.4730],\n",
       "         [0.5252, 0.4748],\n",
       "         [0.5236, 0.4764],\n",
       "         [0.4700, 0.5300],\n",
       "         [0.5101, 0.4899],\n",
       "         [0.5205, 0.4795],\n",
       "         [0.5258, 0.4742],\n",
       "         [0.4900, 0.5100],\n",
       "         [0.5057, 0.4943],\n",
       "         [0.4709, 0.5291],\n",
       "         [0.5184, 0.4816],\n",
       "         [0.5226, 0.4774],\n",
       "         [0.5062, 0.4938],\n",
       "         [0.4951, 0.5049],\n",
       "         [0.4956, 0.5044],\n",
       "         [0.5240, 0.4760],\n",
       "         [0.5396, 0.4604],\n",
       "         [0.5095, 0.4905],\n",
       "         [0.4958, 0.5042],\n",
       "         [0.5114, 0.4886],\n",
       "         [0.5067, 0.4933],\n",
       "         [0.4970, 0.5030],\n",
       "         [0.4828, 0.5172],\n",
       "         [0.4877, 0.5123],\n",
       "         [0.5356, 0.4644],\n",
       "         [0.5317, 0.4683],\n",
       "         [0.5512, 0.4488],\n",
       "         [0.5337, 0.4663],\n",
       "         [0.4848, 0.5152],\n",
       "         [0.4914, 0.5086],\n",
       "         [0.5126, 0.4874],\n",
       "         [0.4945, 0.5055],\n",
       "         [0.4842, 0.5158],\n",
       "         [0.4997, 0.5003],\n",
       "         [0.5133, 0.4867],\n",
       "         [0.5550, 0.4450],\n",
       "         [0.4630, 0.5370],\n",
       "         [0.5090, 0.4910],\n",
       "         [0.4987, 0.5013],\n",
       "         [0.5206, 0.4794],\n",
       "         [0.4916, 0.5084],\n",
       "         [0.4760, 0.5240],\n",
       "         [0.5062, 0.4938],\n",
       "         [0.5330, 0.4670],\n",
       "         [0.5230, 0.4770],\n",
       "         [0.5062, 0.4938],\n",
       "         [0.5097, 0.4903],\n",
       "         [0.5597, 0.4403],\n",
       "         [0.5091, 0.4909],\n",
       "         [0.5605, 0.4395],\n",
       "         [0.5442, 0.4558],\n",
       "         [0.5449, 0.4551],\n",
       "         [0.5163, 0.4837],\n",
       "         [0.5105, 0.4895],\n",
       "         [0.5214, 0.4786],\n",
       "         [0.5100, 0.4900],\n",
       "         [0.4959, 0.5041],\n",
       "         [0.5001, 0.4999],\n",
       "         [0.5127, 0.4873],\n",
       "         [0.4931, 0.5069],\n",
       "         [0.4832, 0.5168],\n",
       "         [0.5299, 0.4701],\n",
       "         [0.4974, 0.5026],\n",
       "         [0.5174, 0.4826],\n",
       "         [0.5298, 0.4702],\n",
       "         [0.4620, 0.5380],\n",
       "         [0.5290, 0.4710],\n",
       "         [0.5160, 0.4840],\n",
       "         [0.5447, 0.4553],\n",
       "         [0.4766, 0.5234],\n",
       "         [0.5022, 0.4978],\n",
       "         [0.4911, 0.5089],\n",
       "         [0.5265, 0.4735]]),\n",
       " TensorCategory([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter, CategoryBlock\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_class_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, vocab=vocab, backwards=backwards), CategoryBlock),\n",
    "                    get_x=ColReader('text'), \n",
    "                    get_y=ColReader('label'))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = text_classifier_from_lm(dls, path=path, backwards=backwards)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
