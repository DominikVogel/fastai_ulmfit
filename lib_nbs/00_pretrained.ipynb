{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained\n",
    "\n",
    "> fast.ai ULMFiT helpers to easily use pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from fastai.text.all import SentencePieceTokenizer, language_model_learner, \\\n",
    "                            text_classifier_learner, untar_data, Path, patch, \\\n",
    "                            LMLearner, os, pickle, shutil, AWD_LSTM, accuracy, \\\n",
    "                            Perplexity, delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_config(path):\n",
    "    with open(path/'model.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_pretrained_model(url):\n",
    "    fname = f\"{url.split('/')[-1]}.tgz\"\n",
    "    path = untar_data(url, fname=fname, c_key='model')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_direction(backwards):\n",
    "    return 'bwd' if backwards else 'fwd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert(_get_direction(backwards=False) == 'fwd')\n",
    "assert(_get_direction(backwards=True) == 'bwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `model` and `vocab` files from path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_files(path, backwards=False):\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    try: \n",
    "        model_path = path/direction\n",
    "        model_file = list(model_path.glob(f'*model.pth'))[0]\n",
    "        vocab_file = list(model_path.glob(f'*vocab.pkl'))[0]\n",
    "        fnames = [model_file.absolute(),vocab_file.absolute()]\n",
    "    except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "    fnames = [str(f.parent/f.stem) for f in fnames]\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `tokenizer` from model-config. Tokenizer parameters in `model.json` will be passed to the Tokenizer. As of now SentencePieceTokenizer is hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenizer_from_pretrained(url, backwards=False, **kwargs):\n",
    "    path = _get_pretrained_model(url)\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    tok = None\n",
    "    if config['tokenizer']['class'] == 'SentencePieceTokenizer':\n",
    "        tok = SentencePieceTokenizer(**config['tokenizer']['params'], **kwargs)\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `langauge_model_learner` from pretrained model-URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(language_model_learner)\n",
    "def language_model_from_pretrained(dls, url=None, backwards=False, metrics=None, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_pretrained_model(url)\n",
    "    fnames = _get_model_files(path)\n",
    "    metrics = [accuracy, Perplexity()] if metrics == None else metrics\n",
    "    return language_model_learner(dls, \n",
    "                                  arch, \n",
    "                                  pretrained=True, \n",
    "                                  pretrained_fnames=fnames, \n",
    "                                  metrics=metrics,\n",
    "                                  **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves a trained or fine-tuned language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_path(learn=None, path=None):\n",
    "    path = (learn.path/learn.model_dir) if not path else Path(path)\n",
    "    if not path.exists(): os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the following model files to `path`:\n",
    "- Model (`lm_model.pth`)\n",
    "- Encoder (`lm_encoder.pth`)\n",
    "- Vocab from dataloaders (`lm_vocab.pkl`)\n",
    "- SentencePieceModel (`spm/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def save_lm(x:LMLearner, path=None, with_encoder=True):\n",
    "    path = _get_model_path(x, path)\n",
    "    x.to_fp32()\n",
    "    # save model\n",
    "    x.save((path/'lm_model').absolute(), with_opt=False)\n",
    "    \n",
    "    # save encoder\n",
    "    if with_encoder:\n",
    "        x.save_encoder((path/'lm_encoder').absolute())\n",
    "\n",
    "    # save vocab\n",
    "    with open((path/'lm_vocab.pkl').absolute(), 'wb') as f:\n",
    "        pickle.dump(x.dls.vocab, f)\n",
    "        \n",
    "    # copy SPM if path not spm path\n",
    "    spm_path = Path(x.dls.tok.cache_dir)\n",
    "    if path.absolute() != spm_path.absolute():\n",
    "        target_path = path/'spm'\n",
    "        if not target_path.exists(): os.makedirs(target_path, exist_ok=True)\n",
    "        shutil.copyfile(spm_path/'spm.model', target_path/'spm.model')\n",
    "        shutil.copyfile(spm_path/'spm.vocab', target_path/'spm.vocab')\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def vocab_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)\n",
    "#    with open((path/'lm_vocab.pkl').absolute(), 'rb') as f:\n",
    "#        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def spm_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `text_classifier_learner` from fine-tuned model path (saved with `learn.save_lm()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(text_classifier_learner)\n",
    "def text_classifier_from_lm(dls, path=None, backwards=False, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_model_path(path=path)\n",
    "    learn = text_classifier_learner(dls, arch, pretrained=False, **kwargs)\n",
    "    learn.load_encoder((path/'lm_encoder').absolute())\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests - Tokenizer, LM and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "url = 'http://localhost:8080/ulmfit-dewiki'\n",
    "tok = tokenizer_from_pretrained(url)\n",
    "assert(tok.vocab_sz == 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/ulmfit/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.504971</td>\n",
       "      <td>6.521173</td>\n",
       "      <td>0.169837</td>\n",
       "      <td>679.374512</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_lm_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, is_lm=True, backwards=backwards)),\n",
    "                    get_x=ColReader('text'), \n",
    "                    splitter=RandomSplitter(valid_pct=0.1, seed=42))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = language_model_from_pretrained(dls, url=url, backwards=backwards)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "path = learn.save_lm()\n",
    "vocab = learn.dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/ulmfit/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.737829</td>\n",
       "      <td>0.693080</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5207, 0.4793],\n",
       "         [0.4851, 0.5149],\n",
       "         [0.5548, 0.4452],\n",
       "         [0.5162, 0.4838],\n",
       "         [0.5283, 0.4717],\n",
       "         [0.5159, 0.4841],\n",
       "         [0.5616, 0.4384],\n",
       "         [0.5148, 0.4852],\n",
       "         [0.5429, 0.4571],\n",
       "         [0.5390, 0.4610],\n",
       "         [0.5448, 0.4552],\n",
       "         [0.4894, 0.5106],\n",
       "         [0.4974, 0.5026],\n",
       "         [0.4913, 0.5087],\n",
       "         [0.4934, 0.5066],\n",
       "         [0.5321, 0.4679],\n",
       "         [0.5187, 0.4813],\n",
       "         [0.5144, 0.4856],\n",
       "         [0.4850, 0.5150],\n",
       "         [0.5156, 0.4844],\n",
       "         [0.4975, 0.5025],\n",
       "         [0.5126, 0.4874],\n",
       "         [0.4875, 0.5125],\n",
       "         [0.4996, 0.5004],\n",
       "         [0.4993, 0.5007],\n",
       "         [0.5608, 0.4392],\n",
       "         [0.5059, 0.4941],\n",
       "         [0.5372, 0.4628],\n",
       "         [0.5267, 0.4733],\n",
       "         [0.5041, 0.4959],\n",
       "         [0.5775, 0.4225],\n",
       "         [0.4946, 0.5054],\n",
       "         [0.4977, 0.5023],\n",
       "         [0.5091, 0.4909],\n",
       "         [0.5064, 0.4936],\n",
       "         [0.5321, 0.4679],\n",
       "         [0.5233, 0.4767],\n",
       "         [0.5643, 0.4357],\n",
       "         [0.4985, 0.5015],\n",
       "         [0.5467, 0.4533],\n",
       "         [0.4750, 0.5250],\n",
       "         [0.5055, 0.4945],\n",
       "         [0.5554, 0.4446],\n",
       "         [0.5374, 0.4626],\n",
       "         [0.4931, 0.5069],\n",
       "         [0.5116, 0.4884],\n",
       "         [0.4768, 0.5232],\n",
       "         [0.5257, 0.4743],\n",
       "         [0.5014, 0.4986],\n",
       "         [0.5110, 0.4890],\n",
       "         [0.5037, 0.4963],\n",
       "         [0.5215, 0.4785],\n",
       "         [0.4997, 0.5003],\n",
       "         [0.5098, 0.4902],\n",
       "         [0.5390, 0.4610],\n",
       "         [0.5131, 0.4869],\n",
       "         [0.4972, 0.5028],\n",
       "         [0.5213, 0.4787],\n",
       "         [0.4991, 0.5009],\n",
       "         [0.5533, 0.4467],\n",
       "         [0.4719, 0.5281],\n",
       "         [0.5004, 0.4996],\n",
       "         [0.5140, 0.4860],\n",
       "         [0.5932, 0.4068],\n",
       "         [0.4896, 0.5104],\n",
       "         [0.5058, 0.4942],\n",
       "         [0.4949, 0.5051],\n",
       "         [0.5090, 0.4910],\n",
       "         [0.5038, 0.4962],\n",
       "         [0.5654, 0.4346],\n",
       "         [0.5196, 0.4804],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.4985, 0.5015],\n",
       "         [0.5194, 0.4806],\n",
       "         [0.5294, 0.4706],\n",
       "         [0.5251, 0.4749],\n",
       "         [0.4826, 0.5174],\n",
       "         [0.5008, 0.4992],\n",
       "         [0.5144, 0.4856],\n",
       "         [0.5568, 0.4432],\n",
       "         [0.5114, 0.4886],\n",
       "         [0.5236, 0.4764],\n",
       "         [0.5108, 0.4892],\n",
       "         [0.5331, 0.4669],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.4923, 0.5077],\n",
       "         [0.5275, 0.4725],\n",
       "         [0.4750, 0.5250],\n",
       "         [0.5092, 0.4908],\n",
       "         [0.5170, 0.4830],\n",
       "         [0.4748, 0.5252],\n",
       "         [0.5156, 0.4844],\n",
       "         [0.4914, 0.5086],\n",
       "         [0.5023, 0.4977],\n",
       "         [0.5012, 0.4988],\n",
       "         [0.4888, 0.5112],\n",
       "         [0.5434, 0.4566],\n",
       "         [0.5394, 0.4606],\n",
       "         [0.4765, 0.5235],\n",
       "         [0.5163, 0.4837],\n",
       "         [0.4990, 0.5010],\n",
       "         [0.4842, 0.5158],\n",
       "         [0.5192, 0.4808],\n",
       "         [0.5238, 0.4762],\n",
       "         [0.5087, 0.4913],\n",
       "         [0.5265, 0.4735],\n",
       "         [0.5060, 0.4940],\n",
       "         [0.5486, 0.4514],\n",
       "         [0.5094, 0.4906],\n",
       "         [0.5184, 0.4816],\n",
       "         [0.4742, 0.5258],\n",
       "         [0.5239, 0.4761],\n",
       "         [0.4952, 0.5048],\n",
       "         [0.5150, 0.4850],\n",
       "         [0.5063, 0.4937],\n",
       "         [0.5026, 0.4974],\n",
       "         [0.5088, 0.4912],\n",
       "         [0.5166, 0.4834],\n",
       "         [0.5535, 0.4465],\n",
       "         [0.5203, 0.4797],\n",
       "         [0.4936, 0.5064],\n",
       "         [0.5550, 0.4450],\n",
       "         [0.5106, 0.4894],\n",
       "         [0.5557, 0.4443],\n",
       "         [0.5041, 0.4959],\n",
       "         [0.5364, 0.4636],\n",
       "         [0.5342, 0.4658],\n",
       "         [0.5208, 0.4792],\n",
       "         [0.5016, 0.4984],\n",
       "         [0.5220, 0.4780],\n",
       "         [0.5236, 0.4764],\n",
       "         [0.5341, 0.4659],\n",
       "         [0.5251, 0.4749],\n",
       "         [0.5267, 0.4733],\n",
       "         [0.4743, 0.5257],\n",
       "         [0.5155, 0.4845],\n",
       "         [0.5327, 0.4673],\n",
       "         [0.5247, 0.4753],\n",
       "         [0.4894, 0.5106],\n",
       "         [0.5038, 0.4962],\n",
       "         [0.4885, 0.5115],\n",
       "         [0.5098, 0.4902],\n",
       "         [0.5186, 0.4814],\n",
       "         [0.4931, 0.5069],\n",
       "         [0.4977, 0.5023],\n",
       "         [0.4957, 0.5043],\n",
       "         [0.5211, 0.4789],\n",
       "         [0.5578, 0.4422],\n",
       "         [0.5126, 0.4874],\n",
       "         [0.4901, 0.5099],\n",
       "         [0.5134, 0.4866],\n",
       "         [0.5034, 0.4966],\n",
       "         [0.4965, 0.5035],\n",
       "         [0.4851, 0.5149],\n",
       "         [0.4981, 0.5019],\n",
       "         [0.5393, 0.4607],\n",
       "         [0.5439, 0.4561],\n",
       "         [0.5484, 0.4516],\n",
       "         [0.5281, 0.4719],\n",
       "         [0.4942, 0.5058],\n",
       "         [0.4849, 0.5151],\n",
       "         [0.5117, 0.4883],\n",
       "         [0.4905, 0.5095],\n",
       "         [0.4809, 0.5191],\n",
       "         [0.4977, 0.5023],\n",
       "         [0.5076, 0.4924],\n",
       "         [0.5492, 0.4508],\n",
       "         [0.4695, 0.5305],\n",
       "         [0.5125, 0.4875],\n",
       "         [0.4873, 0.5127],\n",
       "         [0.5178, 0.4822],\n",
       "         [0.4769, 0.5231],\n",
       "         [0.4798, 0.5202],\n",
       "         [0.5083, 0.4917],\n",
       "         [0.5326, 0.4674],\n",
       "         [0.5176, 0.4824],\n",
       "         [0.5151, 0.4849],\n",
       "         [0.5160, 0.4840],\n",
       "         [0.5572, 0.4428],\n",
       "         [0.5103, 0.4897],\n",
       "         [0.5716, 0.4284],\n",
       "         [0.5349, 0.4651],\n",
       "         [0.5358, 0.4642],\n",
       "         [0.5109, 0.4891],\n",
       "         [0.5051, 0.4949],\n",
       "         [0.5239, 0.4761],\n",
       "         [0.5108, 0.4892],\n",
       "         [0.5167, 0.4833],\n",
       "         [0.4825, 0.5175],\n",
       "         [0.5103, 0.4897],\n",
       "         [0.4878, 0.5122],\n",
       "         [0.4772, 0.5228],\n",
       "         [0.5271, 0.4729],\n",
       "         [0.5071, 0.4929],\n",
       "         [0.5113, 0.4887],\n",
       "         [0.5229, 0.4771],\n",
       "         [0.4598, 0.5402],\n",
       "         [0.5295, 0.4705],\n",
       "         [0.5135, 0.4865],\n",
       "         [0.5548, 0.4452],\n",
       "         [0.4875, 0.5125],\n",
       "         [0.4915, 0.5085],\n",
       "         [0.4965, 0.5035],\n",
       "         [0.5209, 0.4791]]),\n",
       " TensorCategory([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter, CategoryBlock\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_class_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, vocab=vocab, backwards=backwards), CategoryBlock),\n",
    "                    get_x=ColReader('text'), \n",
    "                    get_y=ColReader('label'))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = text_classifier_from_lm(dls, path=path, backwards=backwards)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
