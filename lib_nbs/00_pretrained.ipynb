{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained\n",
    "\n",
    "> fast.ai ULMFiT helpers to easily use pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from fastai.text.all import SentencePieceTokenizer, SpacyTokenizer, language_model_learner, \\\n",
    "                            text_classifier_learner, untar_data, Path, patch, \\\n",
    "                            LMLearner, os, pickle, shutil, AWD_LSTM, accuracy, \\\n",
    "                            Perplexity, delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_config(path):\n",
    "    with open(path/'model.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_pretrained_model(url):\n",
    "    fname = f\"{url.split('/')[-1]}.tgz\"\n",
    "    path = untar_data(url, fname=fname, c_key='model')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_direction(backwards):\n",
    "    return 'bwd' if backwards else 'fwd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert(_get_direction(backwards=False) == 'fwd')\n",
    "assert(_get_direction(backwards=True) == 'bwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `model` and `vocab` files from path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_files(path, backwards=False):\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    try: \n",
    "        model_path = path/direction\n",
    "        model_file = list(model_path.glob(f'*model.pth'))[0]\n",
    "        vocab_file = list(model_path.glob(f'*vocab.pkl'))[0]\n",
    "        fnames = [model_file.absolute(),vocab_file.absolute()]\n",
    "    except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "    fnames = [str(f.parent/f.stem) for f in fnames]\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `tokenizer` from model-config. Tokenizer parameters in `model.json` will be passed to the Tokenizer. As of now SentencePiece and Spacy are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenizer_from_pretrained(url, pretrained=False, backwards=False, **kwargs):\n",
    "    path = _get_pretrained_model(url)\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    sp_model=path/'spm'/'spm.model' if pretrained else None\n",
    "    if config['tokenizer']['class'] == 'SentencePieceTokenizer':\n",
    "        tok = SentencePieceTokenizer(**config['tokenizer']['params'], sp_model=sp_model, **kwargs)\n",
    "    elif config['tokenizer']['class'] == 'SpacyTokenizer':\n",
    "        tok = SpacyTokenizer(**config['tokenizer']['params'], **kwargs)\n",
    "    else:\n",
    "        raise ValueError('Tokenizer not supported')\n",
    "    return tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `langauge_model_learner` from pretrained model-URL. All parameters will be passed to `language_model_learner`. The following parameters are set automatically: `arch`, `pretrained` and `pretrained_fnames`. By default `accuracy` and `perplexity` are passed as `metrics`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(language_model_learner)\n",
    "def language_model_from_pretrained(dls, url=None, backwards=False, metrics=None, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_pretrained_model(url)\n",
    "    fnames = _get_model_files(path)\n",
    "    metrics = [accuracy, Perplexity()] if metrics == None else metrics\n",
    "    return language_model_learner(dls, \n",
    "                                  arch, \n",
    "                                  pretrained=True, \n",
    "                                  pretrained_fnames=fnames, \n",
    "                                  metrics=metrics,\n",
    "                                  **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_path(learn=None, path=None):\n",
    "    path = (learn.path/learn.model_dir) if not path else Path(path)\n",
    "    if not path.exists(): os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the following model files to `path`:\n",
    "- Model (`lm_model.pth`)\n",
    "- Encoder (`lm_encoder.pth`)\n",
    "- Vocab from dataloaders (`lm_vocab.pkl`)\n",
    "- SentencePieceModel (`spm/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def save_lm(x:LMLearner, path=None, with_encoder=True):\n",
    "    path = _get_model_path(x, path)\n",
    "    x.to_fp32()\n",
    "    # save model\n",
    "    x.save((path/'lm_model').absolute(), with_opt=False)\n",
    "    \n",
    "    # save encoder\n",
    "    if with_encoder:\n",
    "        x.save_encoder((path/'lm_encoder').absolute())\n",
    "\n",
    "    # save vocab\n",
    "    with open((path/'lm_vocab.pkl').absolute(), 'wb') as f:\n",
    "        pickle.dump(x.dls.vocab, f)\n",
    "       \n",
    "    # save tokenizer if SentencePiece is used\n",
    "    if isinstance(x.dls.tok, SentencePieceTokenizer):\n",
    "        # copy SPM if path not spm path\n",
    "        spm_path = Path(x.dls.tok.cache_dir)\n",
    "        if path.absolute() != spm_path.absolute():\n",
    "            target_path = path/'spm'\n",
    "            if not target_path.exists(): os.makedirs(target_path, exist_ok=True)\n",
    "            shutil.copyfile(spm_path/'spm.model', target_path/'spm.model')\n",
    "            shutil.copyfile(spm_path/'spm.vocab', target_path/'spm.vocab')\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def vocab_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)\n",
    "#    with open((path/'lm_vocab.pkl').absolute(), 'rb') as f:\n",
    "#        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def spm_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `text_classifier_learner` from fine-tuned model path (saved with `learn.save_lm()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(text_classifier_learner)\n",
    "def text_classifier_from_lm(dls, path=None, backwards=False, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_model_path(path=path)\n",
    "    learn = text_classifier_learner(dls, arch, pretrained=False, **kwargs)\n",
    "    learn.load_encoder((path/'lm_encoder').absolute())\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests - Tokenizer, LM and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "url = 'http://localhost:8080/ulmfit-dewiki'\n",
    "tok = tokenizer_from_pretrained(url, pretrained=True)\n",
    "assert(tok.vocab_sz == 15000)\n",
    "assert('ulmfit-dewiki/spm/spm.model' in str(tok.sp_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "tok = tokenizer_from_pretrained(url, pretrained=False)\n",
    "assert(tok.sp_model == None)\n",
    "assert(tok.vocab_sz == 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.440651</td>\n",
       "      <td>6.521347</td>\n",
       "      <td>0.169837</td>\n",
       "      <td>679.493103</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_lm_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, is_lm=True, backwards=backwards)),\n",
    "                    get_x=ColReader('text'), \n",
    "                    splitter=RandomSplitter(valid_pct=0.1, seed=42))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = language_model_from_pretrained(dls, url=url, backwards=backwards)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "path = learn.save_lm()\n",
    "vocab = learn.dls.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.773969</td>\n",
       "      <td>0.685955</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5084, 0.4916],\n",
       "         [0.4719, 0.5281],\n",
       "         [0.5388, 0.4612],\n",
       "         [0.5013, 0.4987],\n",
       "         [0.5037, 0.4963],\n",
       "         [0.4966, 0.5034],\n",
       "         [0.5330, 0.4670],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.5461, 0.4539],\n",
       "         [0.5114, 0.4886],\n",
       "         [0.5471, 0.4529],\n",
       "         [0.4729, 0.5271],\n",
       "         [0.4876, 0.5124],\n",
       "         [0.4873, 0.5127],\n",
       "         [0.4879, 0.5121],\n",
       "         [0.5057, 0.4943],\n",
       "         [0.5063, 0.4937],\n",
       "         [0.4945, 0.5055],\n",
       "         [0.4684, 0.5316],\n",
       "         [0.4992, 0.5008],\n",
       "         [0.4818, 0.5182],\n",
       "         [0.4998, 0.5002],\n",
       "         [0.4930, 0.5070],\n",
       "         [0.5001, 0.4999],\n",
       "         [0.4888, 0.5112],\n",
       "         [0.5351, 0.4649],\n",
       "         [0.5052, 0.4948],\n",
       "         [0.5320, 0.4680],\n",
       "         [0.5020, 0.4980],\n",
       "         [0.4859, 0.5141],\n",
       "         [0.5468, 0.4532],\n",
       "         [0.4762, 0.5238],\n",
       "         [0.4925, 0.5075],\n",
       "         [0.4900, 0.5100],\n",
       "         [0.4985, 0.5015],\n",
       "         [0.5184, 0.4816],\n",
       "         [0.5138, 0.4862],\n",
       "         [0.5509, 0.4491],\n",
       "         [0.4907, 0.5093],\n",
       "         [0.5231, 0.4769],\n",
       "         [0.4609, 0.5391],\n",
       "         [0.4886, 0.5114],\n",
       "         [0.5189, 0.4811],\n",
       "         [0.5248, 0.4752],\n",
       "         [0.4790, 0.5210],\n",
       "         [0.4931, 0.5069],\n",
       "         [0.4571, 0.5429],\n",
       "         [0.5267, 0.4733],\n",
       "         [0.4959, 0.5041],\n",
       "         [0.4871, 0.5129],\n",
       "         [0.4867, 0.5133],\n",
       "         [0.5003, 0.4997],\n",
       "         [0.4882, 0.5118],\n",
       "         [0.5021, 0.4979],\n",
       "         [0.5192, 0.4808],\n",
       "         [0.5055, 0.4945],\n",
       "         [0.4928, 0.5072],\n",
       "         [0.5039, 0.4961],\n",
       "         [0.4749, 0.5251],\n",
       "         [0.5292, 0.4708],\n",
       "         [0.4593, 0.5407],\n",
       "         [0.4879, 0.5121],\n",
       "         [0.5168, 0.4832],\n",
       "         [0.5602, 0.4398],\n",
       "         [0.4842, 0.5158],\n",
       "         [0.4930, 0.5070],\n",
       "         [0.4815, 0.5185],\n",
       "         [0.5126, 0.4874],\n",
       "         [0.4975, 0.5025],\n",
       "         [0.5355, 0.4645],\n",
       "         [0.5056, 0.4944],\n",
       "         [0.4859, 0.5141],\n",
       "         [0.4968, 0.5032],\n",
       "         [0.4977, 0.5023],\n",
       "         [0.5098, 0.4902],\n",
       "         [0.4904, 0.5096],\n",
       "         [0.4726, 0.5274],\n",
       "         [0.4830, 0.5170],\n",
       "         [0.5006, 0.4994],\n",
       "         [0.5448, 0.4552],\n",
       "         [0.4916, 0.5084],\n",
       "         [0.5039, 0.4961],\n",
       "         [0.5031, 0.4969],\n",
       "         [0.5218, 0.4782],\n",
       "         [0.4955, 0.5045],\n",
       "         [0.4719, 0.5281],\n",
       "         [0.5108, 0.4892],\n",
       "         [0.4663, 0.5337],\n",
       "         [0.5044, 0.4956],\n",
       "         [0.5245, 0.4755],\n",
       "         [0.4549, 0.5451],\n",
       "         [0.5045, 0.4955],\n",
       "         [0.4840, 0.5160],\n",
       "         [0.5041, 0.4959],\n",
       "         [0.4792, 0.5208],\n",
       "         [0.4787, 0.5213],\n",
       "         [0.5332, 0.4668],\n",
       "         [0.5340, 0.4660],\n",
       "         [0.4621, 0.5379],\n",
       "         [0.5123, 0.4877],\n",
       "         [0.4866, 0.5134],\n",
       "         [0.4806, 0.5194],\n",
       "         [0.5220, 0.4780],\n",
       "         [0.4956, 0.5044],\n",
       "         [0.5153, 0.4847],\n",
       "         [0.4977, 0.5023],\n",
       "         [0.5030, 0.4970],\n",
       "         [0.5230, 0.4770],\n",
       "         [0.4872, 0.5128],\n",
       "         [0.4958, 0.5042],\n",
       "         [0.4648, 0.5352],\n",
       "         [0.5080, 0.4920],\n",
       "         [0.4825, 0.5175],\n",
       "         [0.4969, 0.5031],\n",
       "         [0.4843, 0.5157],\n",
       "         [0.4959, 0.5041],\n",
       "         [0.5010, 0.4990],\n",
       "         [0.5129, 0.4871],\n",
       "         [0.5315, 0.4685],\n",
       "         [0.5093, 0.4907],\n",
       "         [0.4789, 0.5211],\n",
       "         [0.5314, 0.4686],\n",
       "         [0.5050, 0.4950],\n",
       "         [0.5254, 0.4746],\n",
       "         [0.4637, 0.5363],\n",
       "         [0.5381, 0.4619],\n",
       "         [0.5349, 0.4651],\n",
       "         [0.5069, 0.4931],\n",
       "         [0.4849, 0.5151],\n",
       "         [0.5168, 0.4832],\n",
       "         [0.5056, 0.4944],\n",
       "         [0.5002, 0.4998],\n",
       "         [0.5223, 0.4777],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.4640, 0.5360],\n",
       "         [0.5029, 0.4971],\n",
       "         [0.5097, 0.4903],\n",
       "         [0.5060, 0.4940],\n",
       "         [0.4796, 0.5204],\n",
       "         [0.4997, 0.5003],\n",
       "         [0.4713, 0.5287],\n",
       "         [0.5051, 0.4949],\n",
       "         [0.5031, 0.4969],\n",
       "         [0.4891, 0.5109],\n",
       "         [0.4904, 0.5096],\n",
       "         [0.4725, 0.5275],\n",
       "         [0.5070, 0.4930],\n",
       "         [0.5448, 0.4552],\n",
       "         [0.4968, 0.5032],\n",
       "         [0.4878, 0.5122],\n",
       "         [0.4881, 0.5119],\n",
       "         [0.5027, 0.4973],\n",
       "         [0.4833, 0.5167],\n",
       "         [0.4707, 0.5293],\n",
       "         [0.4751, 0.5249],\n",
       "         [0.5118, 0.4882],\n",
       "         [0.5303, 0.4697],\n",
       "         [0.5424, 0.4576],\n",
       "         [0.5233, 0.4767],\n",
       "         [0.4845, 0.5155],\n",
       "         [0.4739, 0.5261],\n",
       "         [0.5186, 0.4814],\n",
       "         [0.4826, 0.5174],\n",
       "         [0.4813, 0.5187],\n",
       "         [0.4882, 0.5118],\n",
       "         [0.4968, 0.5032],\n",
       "         [0.5309, 0.4691],\n",
       "         [0.4460, 0.5540],\n",
       "         [0.4900, 0.5100],\n",
       "         [0.4819, 0.5181],\n",
       "         [0.4915, 0.5085],\n",
       "         [0.4799, 0.5201],\n",
       "         [0.4672, 0.5328],\n",
       "         [0.5005, 0.4995],\n",
       "         [0.5122, 0.4878],\n",
       "         [0.5102, 0.4898],\n",
       "         [0.4905, 0.5095],\n",
       "         [0.5047, 0.4953],\n",
       "         [0.5550, 0.4450],\n",
       "         [0.4969, 0.5031],\n",
       "         [0.5372, 0.4628],\n",
       "         [0.5232, 0.4768],\n",
       "         [0.5350, 0.4650],\n",
       "         [0.5076, 0.4924],\n",
       "         [0.5072, 0.4928],\n",
       "         [0.5128, 0.4872],\n",
       "         [0.4900, 0.5100],\n",
       "         [0.4999, 0.5001],\n",
       "         [0.4784, 0.5216],\n",
       "         [0.4913, 0.5087],\n",
       "         [0.4809, 0.5191],\n",
       "         [0.4737, 0.5263],\n",
       "         [0.5199, 0.4801],\n",
       "         [0.4893, 0.5107],\n",
       "         [0.5046, 0.4954],\n",
       "         [0.5039, 0.4961],\n",
       "         [0.4524, 0.5476],\n",
       "         [0.5096, 0.4904],\n",
       "         [0.5161, 0.4839],\n",
       "         [0.5307, 0.4693],\n",
       "         [0.4713, 0.5287],\n",
       "         [0.4910, 0.5090],\n",
       "         [0.4846, 0.5154],\n",
       "         [0.5153, 0.4847]]),\n",
       " TensorCategory([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter, CategoryBlock\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_class_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, vocab=vocab, backwards=backwards), CategoryBlock),\n",
    "                    get_x=ColReader('text'), \n",
    "                    get_y=ColReader('label'))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = text_classifier_from_lm(dls, path=path, backwards=backwards)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
