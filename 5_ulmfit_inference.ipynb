{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'de'\n",
    "wiki = f'{lang}wiki'\n",
    "base_path = Path('data')\n",
    "path = base_path/wiki\n",
    "data_path = path/'germeval'\n",
    "class_path = path/'model'/'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load classification learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fwd = f'{class_path}/fwd/export.pkl'\n",
    "learn_fwd = load_learner(path_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bwd = f'{class_path}/bwd/export.pkl'\n",
    "learn_bwd = load_learner(path_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions for simple texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Text: Komisch das die Realitätsverweigerung immer von linken erbärmlichen Correctiv Accounts ausgeht...  \n",
    "label: OFFENSE  \n",
    "label_fine: INSULT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('OFFENSE', tensor(0), tensor([0.9024, 0.0976]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Komisch das die Realitätsverweigerung immer von linken erbärmlichen Correctiv Accounts ausgeht...'\n",
    "pred = learn_fwd.predict(text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on GermEval2019 Task 2 (Fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GermEval2019 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['text','label','label_fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(data_path/'germeval2019/germeval2019GoldLabelsSubtask1_2.txt',\n",
    "                sep ='\\t', names=names, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('@\\w+', '', text)\n",
    "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", text)\n",
    "    text = text.replace('|LBR|', ' ')\n",
    "    text = text.replace('\\\"', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "df_test['text'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorBase(0.7958)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_fwd = learn_fwd.dls.test_dl(df_test, with_labels=True)\n",
    "preds_fwd = learn_fwd.get_preds(dl=dl_fwd)\n",
    "accuracy(*preds_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorBase(0.8001)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_bwd = learn_bwd.dls.test_dl(df_test, with_labels=True)\n",
    "preds_bwd = learn_bwd.get_preds(dl=dl_bwd)\n",
    "accuracy(*preds_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Forward + Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 'macro'\n",
    "precision = Precision(average=avg)\n",
    "recall = Recall(average=avg)\n",
    "f1score = F1Score(average=avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.8034)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = (preds_fwd[0] + preds_bwd[0]) / 2\n",
    "a = accuracy(preds, preds_fwd[1])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7781402318624189"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = precision(torch.argmax(preds, axis=1), preds_fwd[1])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7571792293801928"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = recall(torch.argmax(preds, axis=1), preds_fwd[1])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7657193813923859"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1score(torch.argmax(preds, axis=1), preds_fwd[1])\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    'accuracy': float(a),\n",
    "    'precision': p,\n",
    "    'recall': r,\n",
    "    'f1score': f1\n",
    "}\n",
    "\n",
    "with open(f'{class_path}/inference_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreation with fastinference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [fastinference](https://muellerzr.github.io/fastinference/)\n",
    "\n",
    "`intrinsic_attention()` shows which tokens contribute most to the classification.   \n",
    "Red tokens = small contribution  \n",
    "Grenn tokens = high contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: monospace;\"><span title=\"0.031\" style=\"background-color: rgba(178, 13, 38, 0.5);\">▁xxbos</span> <span title=\"0.027\" style=\"background-color: rgba(176, 11, 38, 0.5);\">▁xxmaj</span> <span title=\"0.201\" style=\"background-color: rgba(244, 109, 67, 0.5);\">▁komisch</span> <span title=\"0.056\" style=\"background-color: rgba(192, 26, 38, 0.5);\">▁das</span> <span title=\"0.039\" style=\"background-color: rgba(184, 18, 38, 0.5);\">▁die</span> <span title=\"0.044\" style=\"background-color: rgba(186, 20, 38, 0.5);\">▁xxmaj</span> <span title=\"0.235\" style=\"background-color: rgba(247, 131, 77, 0.5);\">▁realität</span> <span title=\"0.075\" style=\"background-color: rgba(202, 35, 38, 0.5);\">s</span> <span title=\"0.218\" style=\"background-color: rgba(245, 119, 71, 0.5);\">verweiger</span> <span title=\"0.047\" style=\"background-color: rgba(188, 22, 38, 0.5);\">ung</span> <span title=\"0.109\" style=\"background-color: rgba(217, 53, 41, 0.5);\">▁immer</span> <span title=\"0.101\" style=\"background-color: rgba(214, 47, 38, 0.5);\">▁von</span> <span title=\"0.439\" style=\"background-color: rgba(254, 236, 159, 0.5);\">▁linken</span> <span title=\"1.000\" style=\"background-color: rgba(0, 104, 55, 0.5);\">▁erbärmlich</span> <span title=\"0.127\" style=\"background-color: rgba(222, 63, 46, 0.5);\">en</span> <span title=\"0.092\" style=\"background-color: rgba(210, 43, 38, 0.5);\">▁xxmaj</span> <span title=\"0.411\" style=\"background-color: rgba(254, 227, 145, 0.5);\">▁correct</span> <span title=\"0.131\" style=\"background-color: rgba(223, 65, 47, 0.5);\">iv</span> <span title=\"0.090\" style=\"background-color: rgba(208, 41, 38, 0.5);\">▁xxmaj</span> <span title=\"0.296\" style=\"background-color: rgba(252, 170, 95, 0.5);\">▁accounts</span> <span title=\"0.037\" style=\"background-color: rgba(182, 16, 38, 0.5);\">▁aus</span> <span title=\"0.132\" style=\"background-color: rgba(223, 65, 47, 0.5);\">geht</span> <span title=\"0.077\" style=\"background-color: rgba(202, 35, 38, 0.5);\">▁...</span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastinference.inference.text import intrinsic_attention\n",
    "learn_fwd.intrinsic_attention(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['OFFENSE'], array([[0.90238917, 0.09761085]], dtype=float32)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_fwd.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
