{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'de'\n",
    "wiki = f'{lang}wiki'\n",
    "base_path = Path('data')\n",
    "path = base_path/wiki\n",
    "data_path = path/'germeval'\n",
    "class_path = path/'model'/'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load classification learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fwd = f'{class_path}/fwd/export.pkl'\n",
    "learn_fwd = load_learner(path_fwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_bwd = f'{class_path}/bwd/export.pkl'\n",
    "learn_bwd = load_learner(path_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get predictions for simple texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Text: Komisch das die Realitätsverweigerung immer von linken erbärmlichen Correctiv Accounts ausgeht...  \n",
    "label: OFFENSE  \n",
    "label_fine: INSULT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('INSULT', tensor(1), tensor([0.3995, 0.5468, 0.0500, 0.0037]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Komisch das die Realitätsverweigerung immer von linken erbärmlichen Correctiv Accounts ausgeht...'\n",
    "pred = learn_fwd.predict(text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on GermEval2019 Task 2 (Fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GermEval2019 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['text','label','label_fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(data_path/'germeval2019/germeval2019GoldLabelsSubtask1_2.txt',\n",
    "                sep ='\\t', names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub('@\\w+', ' ', text)\n",
    "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "df_test['text'] = df_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorBase(0.7301)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_fwd = learn_fwd.dls.test_dl(df_test, with_labels=True)\n",
    "preds_fwd = learn_fwd.get_preds(dl=dl_fwd)\n",
    "accuracy(*preds_fwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorBase(0.7374)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_bwd = learn_bwd.dls.test_dl(df_test, with_labels=True)\n",
    "preds_bwd = learn_bwd.get_preds(dl=dl_bwd)\n",
    "accuracy(*preds_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Forward + Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 'macro'\n",
    "precision = Precision(average=avg)\n",
    "recall = Recall(average=avg)\n",
    "f1score = F1Score(average=avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.7456)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = (preds_fwd[0] + preds_bwd[0]) / 2\n",
    "a = accuracy(preds, preds_fwd[1])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5863656019734342"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = precision(torch.argmax(preds, axis=1), preds_fwd[1])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4930981710582918"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = recall(torch.argmax(preds, axis=1), preds_fwd[1])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5254420614467545"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1score(torch.argmax(preds, axis=1), preds_fwd[1])\n",
    "f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    'accuracy': float(a),\n",
    "    'precision': p,\n",
    "    'recall': r,\n",
    "    'f1score': f1\n",
    "}\n",
    "\n",
    "with open(f'{class_path}/inference_stats.json', 'w') as f:\n",
    "    json.dump(stats, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreation with fastinference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [fastinference](https://muellerzr.github.io/fastinference/)\n",
    "\n",
    "`intrinsic_attention()` shows which tokens contribute most to the classification.   \n",
    "Red tokens = small contribution  \n",
    "Grenn tokens = high contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"font-family: monospace;\"><span title=\"0.037\" style=\"background-color: rgba(182, 16, 38, 0.5);\">▁xxbos</span> <span title=\"0.040\" style=\"background-color: rgba(184, 18, 38, 0.5);\">▁xxmaj</span> <span title=\"0.269\" style=\"background-color: rgba(250, 152, 86, 0.5);\">▁komisch</span> <span title=\"0.090\" style=\"background-color: rgba(208, 41, 38, 0.5);\">▁das</span> <span title=\"0.066\" style=\"background-color: rgba(196, 30, 38, 0.5);\">▁die</span> <span title=\"0.072\" style=\"background-color: rgba(200, 33, 38, 0.5);\">▁xxmaj</span> <span title=\"0.400\" style=\"background-color: rgba(254, 224, 139, 0.5);\">▁realität</span> <span title=\"0.108\" style=\"background-color: rgba(216, 51, 40, 0.5);\">s</span> <span title=\"0.199\" style=\"background-color: rgba(244, 109, 67, 0.5);\">verweiger</span> <span title=\"0.050\" style=\"background-color: rgba(188, 22, 38, 0.5);\">ung</span> <span title=\"0.080\" style=\"background-color: rgba(204, 37, 38, 0.5);\">▁immer</span> <span title=\"0.143\" style=\"background-color: rgba(226, 73, 50, 0.5);\">▁von</span> <span title=\"0.935\" style=\"background-color: rgba(16, 134, 70, 0.5);\">▁linken</span> <span title=\"1.000\" style=\"background-color: rgba(0, 104, 55, 0.5);\">▁erbärmlich</span> <span title=\"0.158\" style=\"background-color: rgba(231, 82, 54, 0.5);\">en</span> <span title=\"0.131\" style=\"background-color: rgba(223, 65, 47, 0.5);\">▁xxmaj</span> <span title=\"0.666\" style=\"background-color: rgba(183, 224, 117, 0.5);\">▁correct</span> <span title=\"0.195\" style=\"background-color: rgba(241, 104, 64, 0.5);\">iv</span> <span title=\"0.091\" style=\"background-color: rgba(210, 43, 38, 0.5);\">▁xxmaj</span> <span title=\"0.272\" style=\"background-color: rgba(250, 154, 88, 0.5);\">▁accounts</span> <span title=\"0.033\" style=\"background-color: rgba(180, 15, 38, 0.5);\">▁aus</span> <span title=\"0.088\" style=\"background-color: rgba(208, 41, 38, 0.5);\">geht</span> <span title=\"0.044\" style=\"background-color: rgba(186, 20, 38, 0.5);\">▁...</span></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastinference.inference.text import intrinsic_attention\n",
    "learn_fwd.intrinsic_attention(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['INSULT'],\n",
       " array([[0.39946818, 0.5468409 , 0.04996781, 0.0037231 ]], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_fwd.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
